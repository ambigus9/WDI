{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Development Indicators - Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryName</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>IndicatorName</th>\n",
       "      <th>IndicatorCode</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>ATG</td>\n",
       "      <td>Adolescent fertility rate (births per 1,000 wo...</td>\n",
       "      <td>SP.ADO.TFRT</td>\n",
       "      <td>1960</td>\n",
       "      <td>126.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>ATG</td>\n",
       "      <td>Age dependency ratio (% of working-age populat...</td>\n",
       "      <td>SP.POP.DPND</td>\n",
       "      <td>1960</td>\n",
       "      <td>88.237117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>ATG</td>\n",
       "      <td>Age dependency ratio, old (% of working-age po...</td>\n",
       "      <td>SP.POP.DPND.OL</td>\n",
       "      <td>1960</td>\n",
       "      <td>7.779958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>ATG</td>\n",
       "      <td>Age dependency ratio, young (% of working-age ...</td>\n",
       "      <td>SP.POP.DPND.YG</td>\n",
       "      <td>1960</td>\n",
       "      <td>80.457159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>ATG</td>\n",
       "      <td>Birth rate, crude (per 1,000 people)</td>\n",
       "      <td>SP.DYN.CBRT.IN</td>\n",
       "      <td>1960</td>\n",
       "      <td>32.920000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CountryName CountryCode  \\\n",
       "0  Antigua and Barbuda         ATG   \n",
       "1  Antigua and Barbuda         ATG   \n",
       "2  Antigua and Barbuda         ATG   \n",
       "3  Antigua and Barbuda         ATG   \n",
       "4  Antigua and Barbuda         ATG   \n",
       "\n",
       "                                       IndicatorName   IndicatorCode  Year  \\\n",
       "0  Adolescent fertility rate (births per 1,000 wo...     SP.ADO.TFRT  1960   \n",
       "1  Age dependency ratio (% of working-age populat...     SP.POP.DPND  1960   \n",
       "2  Age dependency ratio, old (% of working-age po...  SP.POP.DPND.OL  1960   \n",
       "3  Age dependency ratio, young (% of working-age ...  SP.POP.DPND.YG  1960   \n",
       "4               Birth rate, crude (per 1,000 people)  SP.DYN.CBRT.IN  1960   \n",
       "\n",
       "        Value  \n",
       "0  126.144000  \n",
       "1   88.237117  \n",
       "2    7.779958  \n",
       "3   80.457159  \n",
       "4   32.920000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "datos = pd.read_csv(\"suramerica.csv\").drop('Unnamed: 0', 1)\n",
    "paises = datos['CountryCode'].drop_duplicates().values.tolist()\n",
    "\n",
    "preprocessing = 'imput'\n",
    "search = 'original'\n",
    "year_init = 2012\n",
    "year_range = 5\n",
    "look_back = 3\n",
    "years = range(year_init-year_range+1,year_init+1)[::-1]\n",
    "\n",
    "agricultura     = open(\"Indicadores/iagricultura.txt\").read().split(',')\n",
    "ambiente        = open(\"Indicadores/iambiente.txt\").read().split(',')\n",
    "ayuda           = open(\"Indicadores/iayuda.txt\").read().split(',')\n",
    "ciencia         = open(\"Indicadores/iciencia.txt\").read().split(',')\n",
    "clima           = open(\"Indicadores/iclima.txt\").read().split(',')\n",
    "comercio        = open(\"Indicadores/icomercio.txt\").read().split(',')\n",
    "deuda           = open(\"Indicadores/ideuda.txt\").read().split(',')\n",
    "economia        = open(\"Indicadores/ieconomia.txt\").read().split(',')\n",
    "educacion       = open(\"Indicadores/ieducacion.txt\").read().split(',')\n",
    "energia         = open(\"Indicadores/ienergia.txt\").read().split(',')\n",
    "finanzas        = open(\"Indicadores/ifinanzas.txt\").read().split(',')\n",
    "genero          = open(\"Indicadores/igenero.txt\").read().split(',')\n",
    "infraestructura = open(\"Indicadores/iinfraestructura.txt\").read().split(',')\n",
    "pobreza         = open(\"Indicadores/ipobreza.txt\").read().split(',')\n",
    "privado         = open(\"Indicadores/iprivado.txt\").read().split(',')\n",
    "publico         = open(\"Indicadores/ipublico.txt\").read().split(',')\n",
    "salud           = open(\"Indicadores/isalud.txt\").read().split(',')\n",
    "social          = open(\"Indicadores/isocial.txt\").read().split(',')\n",
    "trabajo         = open(\"Indicadores/itrabajo.txt\").read().split(',')\n",
    "urbano          = open(\"Indicadores/iurbano.txt\").read().split(',')\n",
    "\n",
    "\n",
    "conjunto_nombre = ['Agricultura','Ambiente','Ayuda','Ciencia','Clima','Comercio','Deuda','Economia','Educacion',\n",
    "                   'Energia','Finanzas','Genero','Infraestructura','Pobreza','Privado','Publico','Salud','Social',\n",
    "                   'Trabajo','Urbano']\n",
    "conjunto = [agricultura,ambiente,ayuda,ciencia,clima,comercio,deuda,economia,educacion,energia,finanzas,genero,\n",
    "            infraestructura,pobreza,privado,publico,salud,social,trabajo,urbano]\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def r2(y_true, y_predict):\n",
    "    from sklearn.metrics import r2_score\n",
    "    return r2_score(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tabla_base(indicadores):\n",
    "    tab = pd.DataFrame.pivot_table(datos, values='Value', index=['CountryCode', 'Year'], columns=['IndicatorCode']).loc[(paises,years),indicadores].sortlevel([\"CountryCode\",\"Year\"], ascending=[True,False])\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tabla_2_base(indicadores,look_back):\n",
    "    temp_table = []\n",
    "    for i in range(look_back):      \n",
    "        temp_years = range(year_init-year_range-i+1,year_init-i+1)[::-1]\n",
    "        temp_table.append(pd.DataFrame.pivot_table(datos, values='Value', index=['CountryCode', 'Year'], columns=['IndicatorCode']).loc[(paises,temp_years),indicadores].sortlevel([\"CountryCode\",\"Year\"], ascending=[True,False]))\n",
    "    return pd.DataFrame(np.column_stack(temp_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_Universal(estimador, X_train, X_test, y_train, y_test):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    \n",
    "    if(search=='original'):\n",
    "        if(estimador=='DTR'):\n",
    "            estimator = DecisionTreeRegressor()\n",
    "        if(estimador=='RFR'):\n",
    "            estimator = RandomForestRegressor(n_jobs=-1)\n",
    "        if(estimador=='SVR'):\n",
    "            estimator = SVR()\n",
    "    else:\n",
    "        best_params = SearchCV_Universal(estimador, search, X_train, y_train)       \n",
    "\n",
    "        if(estimador=='DTR'):\n",
    "            estimator = DecisionTreeRegressor().set_params(**best_params)\n",
    "        if(estimador=='RFR'):\n",
    "            estimator = RandomForestRegressor(n_jobs=-1).set_params(**best_params)\n",
    "        if(estimador=='SVR'):\n",
    "            estimator = SVR().set_params(**best_params)\n",
    "        \n",
    "    estimator.fit(X_train,y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "        \n",
    "    return r2(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Preprocess(tab1,tab2,y_indicator):\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "       \n",
    "    #Fusionamos la tabla_1 y el indicador y de la tabla_2\n",
    "    tab_fusion = np.column_stack((np.array(tab1)[:,:],np.array(tab2)[:,y_indicator]))    \n",
    "    \n",
    "    #Eliminamos las columnas de NaN descartando Indicadores que no tienen regristros para ningún pais y año deseados\n",
    "    df = pd.DataFrame(tab_fusion)\n",
    "    df = df.dropna(how='all',axis=1)\n",
    "    \n",
    "    #Eliminamos las filas Si el valor a predecir es NaN\n",
    "    df = df.dropna(subset=[df.iloc[:,-1].name])\n",
    "    \n",
    "    if(preprocessing=='zeros'):\n",
    "        #Imputamos los NaN por Zero\n",
    "        df = df.fillna(0)      \n",
    "        \n",
    "    if(preprocessing=='imput'):\n",
    "        #Imputamos los NaN por la media de cada Indicador respectivamente      \n",
    "        impute=Imputer(missing_values=\"NaN\",strategy='mean',axis=0)\n",
    "        impute.fit(df)\n",
    "        df = pd.DataFrame(impute.transform(df))\n",
    "        \n",
    "    # Asignamos X e y, eliminando los indicadores que se correlacionen más (coeficiente > 0.7)\n",
    "    df_ = df.corr()\n",
    "    X = df.drop(df_[df_.iloc[:,-1] > 0.7].index, axis=1)\n",
    "    y = df.iloc[:,-1]\n",
    "\n",
    "    # Normalizamos los datos\n",
    "    sc = StandardScaler()\n",
    "    df_norm = sc.fit_transform(np.column_stack([X,y]))\n",
    "    X = df_norm[:,:-1]\n",
    "    y = df_norm[:,-1]\n",
    "    \n",
    "    # Separamos Train y Test respectivamente para X e y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_Splitter_Optimus(tab1,tab2): \n",
    "    R2_global = list()\n",
    "    for i in range(0,np.shape(tab2.dropna(how='all',axis=1))[1]):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = Preprocess(tab1, tab2, i)\n",
    "\n",
    "        result = estimator_Universal('DTR', X_train, X_test, y_train, y_test)\n",
    "\n",
    "        if(result < 0.9): \n",
    "            temp = estimator_Universal('SVR', X_train, X_test, y_train, y_test)\n",
    "            if(temp < 0.9): \n",
    "                temp2 = estimator_Universal('RFR', X_train, X_test, y_train, y_test)\n",
    "                if (temp2 > temp): \n",
    "                    result = temp2\n",
    "            if(temp > result): \n",
    "                result = temp\n",
    "\n",
    "        R2_global.append(result)\n",
    "    return R2_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SearchCV_Universal(estimador, search, X_train, y_train):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import ShuffleSplit\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.svm import SVR\n",
    "        \n",
    "    if(estimador=='DTR'):\n",
    "        estimator  = DecisionTreeRegressor()\n",
    "        param_grid = {  'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                        'splitter': ['best', 'random']\n",
    "                     }\n",
    "        \n",
    "    if(estimador=='RFR'):\n",
    "        estimator  = RandomForestRegressor()       \n",
    "        param_grid = { \n",
    "                        \"n_estimators\"      : [10,20,30,40],\n",
    "                        \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n",
    "                        \"min_samples_split\" : [2,4,8],\n",
    "                        \"bootstrap\": [True, False],\n",
    "                     }\n",
    "    if(estimador=='SVR'):\n",
    "        estimator  = SVR()\n",
    "        param_grid ={\n",
    "                        'gamma'  : ['auto', 1e-3, 1e-4],\n",
    "                        'C'      : [1, 10, 100, 1000],            \n",
    "                    }\n",
    "\n",
    "    if (search=='random'):\n",
    "        grid = RandomizedSearchCV(estimator, param_grid, n_jobs=-1, cv=ShuffleSplit(test_size=0.2))\n",
    "    if (search=='grid'):\n",
    "        grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=ShuffleSplit(test_size=0.2))\n",
    "        \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filtro(indicadores_1,indicadores_2):\n",
    "    if(indicadores_1!=indicadores_2):\n",
    "        df = pd.DataFrame(indicadores_2)\n",
    "        indicadores_2 = np.array(df.loc[~df.ix[:,0].isin(indicadores_1)]).flatten()\n",
    "    return indicadores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterador_global(indicadores_1,indicadores_2,look_back): \n",
    "    \n",
    "    tab1 = tabla_2_base(indicadores_1,look_back)\n",
    "    tab2 = tabla_base(filtro(indicadores_1,indicadores_2))\n",
    "    \n",
    "    return iter_Splitter_Optimus(tab1,tab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def porcent_result_reg(df,indicadores_1,indicadores_2):\n",
    "    df_temp = df\n",
    "    x = 0.0\n",
    "    for i in range(len(df_temp)):\n",
    "        if(df_temp[i] > 0.9):\n",
    "            x=x+1\n",
    "    porcent = x/len(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    df[df < 0] = 0.0\n",
    "    result = np.array(df.values)   \n",
    "    reg = len(filtro(indicadores_1,indicadores_2))\n",
    "    return porcent,result,reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resultados(df,indicador_1,indicador_2,look_back):\n",
    "    #Buscamos Resultados en base al conjunto de indicadores Base, Target y Look Back\n",
    "    temp = df[(df['Base']==indicador_1)&(df['Target']==indicador_2)&(df['Look Back'] == look_back)]['Results'].values[0]\n",
    "        \n",
    "    # Códigos de los indicadores con registros   \n",
    "    for i in range(len(conjunto_nombre)):\n",
    "        \n",
    "        if indicador_1 == conjunto_nombre[i]:\n",
    "            indicador1 = conjunto[i]\n",
    "        if indicador_2 == conjunto_nombre[i]:\n",
    "            indicador2 = conjunto[i]\n",
    "\n",
    "            df = pd.DataFrame(indicador2)\n",
    "            # Filtro indicadores repetidos\n",
    "            if(indicador1!=indicador2):\n",
    "                indicador2 = np.array(df.loc[~df.ix[:,0].isin(indicador1)]).flatten()\n",
    "            df_zeros = tabla_base(indicador2).dropna(how='all',axis=1)\n",
    "                \n",
    "    icodes = df_zeros.columns.values\n",
    "\n",
    "    # Buscamos Nombres de los Códigos de los Indicadores\n",
    "    nombres = pd.DataFrame(datos).loc[:,['IndicatorName','IndicatorCode']].set_index('IndicatorCode').loc[icodes].drop_duplicates()\n",
    "\n",
    "    # Concatenamos Resultados y Nombres, Reindexando en base a los nombres\n",
    "    df_results = pd.DataFrame(np.column_stack([nombres,temp]),columns=['Indicadores','Resultados'])\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buscar_mejores(df):\n",
    "    return df[df.Resultados > 0.9].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graficar(df,look_back):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    df_temp = df[df['Look Back'] == 1].drop(['Time','Reg','Look Back','Results'], 1)\n",
    "    df_temp = pd.DataFrame(np.array(pd.DataFrame.pivot_table(df_temp, index=[\"Base\"], columns=[\"Target\"])), index=[conjunto_nombre], columns=[conjunto_nombre])\n",
    "    ax = sns.heatmap(df_temp)\n",
    "    plt.title('Look back = '+str(look_back), size=15)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteramos todos los Conjuntos de Indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "resultado_global = []\n",
    "\n",
    "for i in range(len(conjunto)):\n",
    "    for j in range(len(conjunto)):\n",
    "        for k in range(1,look_back+1):\n",
    "            start_time2 = time.time()\n",
    "            porcent , results, reg = porcent_result_reg(iterador_global(conjunto[i],conjunto[j],k),conjunto[i],conjunto[j])\n",
    "            resultado_global.append([conjunto_nombre[i],conjunto_nombre[j],k,reg,porcent,\"%s\" % (time.time() - start_time2),results])\n",
    "            pd.DataFrame(resultado_global, columns=[\"Base\",\"Target\",\"Look Back\",\"Reg\",\"%\",\"Time\",\"Results\"]).to_csv('log_global.csv')\n",
    "\n",
    "\n",
    "df_global = pd.DataFrame(resultado_global, columns=[\"Base\",\"Target\",\"Look Back\",\"Reg\",\"%\",\"Time\",\"Results\"])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "df_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.pivot_table(df_global, index=[\"Base\",\"Target\",\"Reg\"], columns=[\"Look Back\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graficar(df_global,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
