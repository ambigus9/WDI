{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Development Indicators - Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "datos = pd.read_csv(\"suramerica.csv\").drop('Unnamed: 0', 1)\n",
    "paises = datos['CountryCode'].drop_duplicates().values.tolist()\n",
    "\n",
    "preprocessing = 'imput'\n",
    "search = 'original'\n",
    "year_init = 2012\n",
    "year_range = 5\n",
    "look_back = 3\n",
    "years = range(year_init-year_range+1,year_init+1)[::-1]\n",
    "\n",
    "agricultura     = open(\"Indicadores/iagricultura.txt\").read().split(',')\n",
    "ambiente        = open(\"Indicadores/iambiente.txt\").read().split(',')\n",
    "ayuda           = open(\"Indicadores/iayuda.txt\").read().split(',')\n",
    "ciencia         = open(\"Indicadores/iciencia.txt\").read().split(',')\n",
    "clima           = open(\"Indicadores/iclima.txt\").read().split(',')\n",
    "comercio        = open(\"Indicadores/icomercio.txt\").read().split(',')\n",
    "deuda           = open(\"Indicadores/ideuda.txt\").read().split(',')\n",
    "economia        = open(\"Indicadores/ieconomia.txt\").read().split(',')\n",
    "educacion       = open(\"Indicadores/ieducacion.txt\").read().split(',')\n",
    "energia         = open(\"Indicadores/ienergia.txt\").read().split(',')\n",
    "finanzas        = open(\"Indicadores/ifinanzas.txt\").read().split(',')\n",
    "genero          = open(\"Indicadores/igenero.txt\").read().split(',')\n",
    "infraestructura = open(\"Indicadores/iinfraestructura.txt\").read().split(',')\n",
    "pobreza         = open(\"Indicadores/ipobreza.txt\").read().split(',')\n",
    "privado         = open(\"Indicadores/iprivado.txt\").read().split(',')\n",
    "publico         = open(\"Indicadores/ipublico.txt\").read().split(',')\n",
    "salud           = open(\"Indicadores/isalud.txt\").read().split(',')\n",
    "social          = open(\"Indicadores/isocial.txt\").read().split(',')\n",
    "trabajo         = open(\"Indicadores/itrabajo.txt\").read().split(',')\n",
    "urbano          = open(\"Indicadores/iurbano.txt\").read().split(',')\n",
    "\n",
    "\n",
    "conjunto_nombre = ['Agricultura','Ambiente','Ayuda','Ciencia','Clima','Comercio','Deuda','Economia','Educacion',\n",
    "                   'Energia','Finanzas','Genero','Infraestructura','Pobreza','Privado','Publico','Salud','Social',\n",
    "                   'Trabajo','Urbano']\n",
    "\n",
    "conjunto = [agricultura,ambiente,ayuda,ciencia,clima,comercio,deuda,economia,educacion,energia,finanzas,genero,\n",
    "            infraestructura,pobreza,privado,publico,salud,social,trabajo,urbano]\n",
    "\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def r2(y_true, y_predict):\n",
    "    from sklearn.metrics import r2_score\n",
    "    return r2_score(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tabla_base(indicadores):\n",
    "    tab = pd.DataFrame.pivot_table(datos, values='Value', index=['CountryCode', 'Year'], columns=['IndicatorCode']).loc[(paises,years),indicadores].sortlevel([\"CountryCode\",\"Year\"], ascending=[True,False])\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tabla_2_base(indicadores,look_back):\n",
    "    temp_table = []\n",
    "    for i in range(look_back):      \n",
    "        temp_years = range(year_init-year_range-i+1,year_init-i+1)[::-1]\n",
    "        temp_table.append(pd.DataFrame.pivot_table(datos, values='Value', index=['CountryCode', 'Year'], columns=['IndicatorCode']).loc[(paises,temp_years),indicadores].sortlevel([\"CountryCode\",\"Year\"], ascending=[True,False]))\n",
    "    return pd.DataFrame(np.column_stack(temp_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_Universal(estimador, X_train, X_test, y_train, y_test):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    \n",
    "    if(search=='original'):\n",
    "        if(estimador=='DTR'):\n",
    "            estimator = DecisionTreeRegressor()\n",
    "        if(estimador=='RFR'):\n",
    "            estimator = RandomForestRegressor(n_jobs=-1)\n",
    "        if(estimador=='SVR'):\n",
    "            estimator = SVR()\n",
    "    else:\n",
    "        best_params = SearchCV_Universal(estimador, search, X_train, y_train)       \n",
    "\n",
    "        if(estimador=='DTR'):\n",
    "            estimator = DecisionTreeRegressor().set_params(**best_params)\n",
    "        if(estimador=='RFR'):\n",
    "            estimator = RandomForestRegressor(n_jobs=-1).set_params(**best_params)\n",
    "        if(estimador=='SVR'):\n",
    "            estimator = SVR().set_params(**best_params)\n",
    "        \n",
    "    estimator.fit(X_train,y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    \n",
    "    predicciones.append([y_test,y_predict])\n",
    "    \n",
    "    return r2(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Preprocess(tab1,tab2,y_indicator):\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #Eliminamos las columnas de NaN descartando Indicadores que no tienen regristros para ningún pais y año deseados\n",
    "    tab1 = tab1.dropna(how='all',axis=1)\n",
    "    tab2 = tab2.dropna(how='all',axis=1)\n",
    "\n",
    "    if(preprocessing=='zeros'):\n",
    "        #Imputamos los NaN por Zero\n",
    "        tab1 = tab1.fillna(0)      \n",
    "\n",
    "    if(preprocessing=='imput'):\n",
    "        #Imputamos los NaN por la media de cada Indicador respectivamente      \n",
    "        impute=Imputer(missing_values=\"NaN\",strategy='mean',axis=0)\n",
    "        impute.fit(tab1)\n",
    "        tab1 = pd.DataFrame(impute.transform(tab1))\n",
    "\n",
    "    #Fusionamos la tabla_1 y el indicador y de la tabla_2\n",
    "    tab_fusion = pd.DataFrame(np.column_stack((np.array(tab1)[:,:],np.array(tab2)[:,y_indicator])))\n",
    "\n",
    "    #Eliminamos las filas Si el valor a predecir es NaN\n",
    "    tab_fusion = tab_fusion.dropna(subset=[tab_fusion.iloc[:,-1].name])\n",
    "\n",
    "    # Asignamos X e y, eliminando los indicadores que se correlacionen más con el indicador a predecir (coeficiente > 0.7)\n",
    "    tab_fusion_corr = tab_fusion.corr()\n",
    "    X = tab_fusion.drop(tab_fusion_corr[tab_fusion_corr.iloc[:,-1] > 0.7].index, axis=1)\n",
    "    y = tab_fusion.iloc[:,-1]\n",
    "\n",
    "    # Normalizamos los datos\n",
    "    sc = StandardScaler()\n",
    "    tab_fusion_norm = sc.fit_transform(np.column_stack([X,y]))\n",
    "    X = tab_fusion_norm[:,:-1]\n",
    "    y = tab_fusion_norm[:,-1]\n",
    "\n",
    "    # Separamos Train y Test respectivamente para X e y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_Splitter_Optimus(tab1,tab2): \n",
    "    R2_global = list()\n",
    "    for i in range(0,np.shape(tab2.dropna(how='all',axis=1))[1]):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = Preprocess(tab1, tab2, i)\n",
    "\n",
    "        result = estimator_Universal('DTR', X_train, X_test, y_train, y_test)\n",
    "  \n",
    "        if(result < 0.9): \n",
    "            temp = estimator_Universal('SVR', X_train, X_test, y_train, y_test)\n",
    "            \n",
    "            if(temp < 0.9): \n",
    "                temp2 = estimator_Universal('RFR', X_train, X_test, y_train, y_test)\n",
    "                \n",
    "                if (temp2 > temp): \n",
    "                    result = temp2\n",
    "                    \n",
    "            if(temp > result): \n",
    "                result = temp\n",
    "        \n",
    "        R2_global.append(result)\n",
    "    return R2_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SearchCV_Universal(estimador, search, X_train, y_train):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import ShuffleSplit\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.svm import SVR\n",
    "        \n",
    "    if(estimador=='DTR'):\n",
    "        estimator  = DecisionTreeRegressor()\n",
    "        param_grid = {  'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                        'splitter': ['best', 'random']\n",
    "                     }\n",
    "        \n",
    "    if(estimador=='RFR'):\n",
    "        estimator  = RandomForestRegressor()       \n",
    "        param_grid = { \n",
    "                        \"n_estimators\"      : [10,20,30,40],\n",
    "                        \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n",
    "                        \"min_samples_split\" : [2,4,8],\n",
    "                        \"bootstrap\": [True, False],\n",
    "                     }\n",
    "    if(estimador=='SVR'):\n",
    "        estimator  = SVR()\n",
    "        param_grid ={\n",
    "                        'gamma'  : ['auto', 1e-3, 1e-4],\n",
    "                        'C'      : [1, 10, 100, 1000],            \n",
    "                    }\n",
    "\n",
    "    if (search=='random'):\n",
    "        grid = RandomizedSearchCV(estimator, param_grid, n_jobs=-1, cv=ShuffleSplit(test_size=0.2))\n",
    "    if (search=='grid'):\n",
    "        grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=ShuffleSplit(test_size=0.2))\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "        \n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filtro(indicadores_1,indicadores_2):\n",
    "    if(indicadores_1!=indicadores_2):\n",
    "        df = pd.DataFrame(indicadores_2)\n",
    "        indicadores_2 = np.array(df.loc[~df.ix[:,0].isin(indicadores_1)]).flatten()\n",
    "    return indicadores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterador_global(indicadores_1,indicadores_2,look_back): \n",
    "    \n",
    "    tab1 = tabla_2_base(indicadores_1,look_back)\n",
    "    tab2 = tabla_base(filtro(indicadores_1,indicadores_2))\n",
    "    \n",
    "    return iter_Splitter_Optimus(tab1,tab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def porcent_result_reg(df,indicadores_1,indicadores_2):\n",
    "    df = pd.DataFrame(df)\n",
    "    df[df < 0] = 0.0\n",
    "    porcent = df.mean().values[0]\n",
    "    result = np.array(df.values)   \n",
    "    reg = len(filtro(indicadores_1,indicadores_2))\n",
    "    return porcent,result,reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteramos todos los Conjuntos de Indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultado_global = []\n",
    "predicciones = []\n",
    "\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        for k in range(1,look_back+1):\n",
    "            start_time = time.time()\n",
    "            porcent , results, reg = porcent_result_reg(iterador_global(conjunto[i],conjunto[j],k),conjunto[i],conjunto[j])\n",
    "            resultado_global.append([conjunto_nombre[i],conjunto_nombre[j],k,reg,porcent,\"%s\" % (time.time() - start_time),results])\n",
    "            pd.DataFrame(resultado_global, columns=[\"Base\",\"Target\",\"Look Back\",\"Reg\",\"%\",\"Time\",\"Results\"]).to_csv('suramerica_absolute.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
